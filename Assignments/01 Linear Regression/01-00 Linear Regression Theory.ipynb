{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [What is Linear Regression?](#lr-what-is)\n",
    "- [Hypothesis](#lr-hypothesis)\n",
    "- [Cost function](#lr-cost-function)\n",
    "- [Train Dataset](#lr-dataset)\n",
    "- [Training](#lr-training)\n",
    "- [Gradient Descent Implementation](#lr-gd-impl)\n",
    "- [Evaluation](#lr-evaluation)\n",
    "- [SKLEARN Solution](#lr-sklearn)\n",
    "- [Results](#lr-results)\n",
    "- [Resources](#lr-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Linear Regression? <a class=\"anchor\" id=\"lr-what-is\"></a>\n",
    "Linear regression is a model that finds a relationship between one or more features (independent variables) and a continuous target variable (dependent variable).\n",
    "When there is only one feature it is called <b>Uni-Variate Linear Regression</b> and if there are multiple features, it is called <b>Multiple Linear Regression</b>.\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Linear_regression\">Linear Regression</a> is one of existing models used in <b>Statistics</b> for <b>Regression Analysis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Hypothesis <a class=\"anchor\" id=\"lr-hypothesis\"></a>\n",
    "Hypothesis is a function, that is used to train model and eventually predict outcome based on new input.\n",
    "\n",
    "$$ Y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n $$\n",
    "or\n",
    "$$ h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n $$\n",
    "\n",
    "- $Y$ or $h(x)$ - Predicted Value or Hypothesis\n",
    "- $\\theta_0$ - bias\n",
    "- $\\theta_1, ..., \\theta_n$ - model parameters\n",
    "- $x_1, ..., x_n$ - feature values\n",
    "\n",
    "Hypothesis $Y$ also can be represented as:\n",
    "\n",
    "$$ Y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n = \\sum_{n=1}^{m}\\theta_{i}x_{i} = \\theta^{T}x$$\n",
    "\n",
    "- $m$ - total number of examples in our dataset\n",
    "- $\\theta^{T}$ - is the modelâ€™s parameter vector including the bias term $\\theta_{0}$\n",
    "- $x$ - is the feature vector with $x_{0} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function <a class=\"anchor\" id=\"lr-cost-function\"></a>\n",
    "\n",
    "Cost function is used to define and measure the error of our model. We will define it as the sum of squares of errors.\n",
    "\n",
    "$$ J(\\theta) = \\dfrac{1}{2m}\\sum_{i = 1}^{m}(h(x^i) - y^i)^2$$\n",
    "\n",
    "<b><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">Gradient descent</a></b> is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset <a class=\"anchor\" id=\"lr-dataset\"></a>\n",
    "\n",
    "For the purpose if this notebook, I will generate bunch of random digits that will be used as 'dataset' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case if we will need later on just run some blocks that depend on NumPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Also I will import dataset split function, because I'm lazy.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c213c0bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate random dataset\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 3 * x + np.random.rand(100, 1)\n",
    "\n",
    "# Build distribution graph for newly generated numbers\n",
    "plt.scatter(x,y,s=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Data Model <a class=\"anchor\" id=\"lr-training\"></a>\n",
    "\n",
    "Basically we need to find parameters so that model best fit the data. In this case our randomly generated data. \n",
    "\n",
    "The line for which the the <i>error</i> between the <b>predicted values</b> and the <b>observed values</b> is <b>minimum</b> is called the best fit line or the <b>regression line</b>. \n",
    "\n",
    "By the way, this <i>errors</i> can also be called in some references as <b>residuals</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradien Descent Implementation <a class=\"anchor\" id=\"lr-gd-impl\"></a>\n",
    "\n",
    "Let's try to implement simple gradient descent based fit function.\n",
    "We were suggested to try to implement this ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionWithGradientDescent:\n",
    "    \"\"\"Linear Regression Using Gradient Descent.\n",
    "    Local variables\n",
    "    ----------\n",
    "    eta : float\n",
    "        Defines learning rate\n",
    "    iterations : int\n",
    "        Numner of passes over the training set\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    coeff_ : final weights after fitting the model\n",
    "    cost_ : total error of the model after each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta = 0.05, iterations = 1000):\n",
    "        self.eta = eta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit function\n",
    "        Input variables\n",
    "        ----------\n",
    "        x : array-like, shape = [n_samples, n_features]\n",
    "            Training samples\n",
    "        y : array-like, shape = [n_samples, n_target_values]\n",
    "            Target values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.coeff_ = np.zeros((x.shape[1], 1))\n",
    "        m = x.shape[0]\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            y_pred = np.dot(x, self.coeff_)\n",
    "            residuals = y_pred - y\n",
    "            gradient_vector = np.dot(x.T, residuals)\n",
    "            self.coeff_ -= (self.eta / m) * gradient_vector\n",
    "            cost = np.sum((residuals ** 2)) / (2 * m)\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Prediction function that can be used after model is trained.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like, shape = [n_samples, n_features]\n",
    "            Test samples\n",
    "        Returns\n",
    "        -------\n",
    "        Predicted value\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.coeff_)\n",
    "    \n",
    "    def get_coefficients(self):\n",
    "        print (\"Coefficient after model training:\", self.coeff_)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values based on model training:\n",
      " [[3.12522492]\n",
      " [1.73639969]\n",
      " [0.80085048]\n",
      " [1.98648822]\n",
      " [2.81410964]\n",
      " [2.97119287]\n",
      " [3.01381148]\n",
      " [3.72524684]\n",
      " [3.16948325]\n",
      " [2.35111543]\n",
      " [0.07654217]\n",
      " [2.34844507]\n",
      " [3.71702877]\n",
      " [1.36851683]\n",
      " [0.60513983]\n",
      " [0.07152599]\n",
      " [2.65563018]\n",
      " [0.96418972]\n",
      " [2.16234063]\n",
      " [0.36581196]] \n",
      "\n",
      "Actual values based on test set:\n",
      " [[2.47469377]\n",
      " [1.50392506]\n",
      " [0.73137457]\n",
      " [2.2257185 ]\n",
      " [3.01148844]\n",
      " [2.98557773]\n",
      " [2.82230049]\n",
      " [3.66110931]\n",
      " [3.07913241]\n",
      " [2.44377925]\n",
      " [0.31467912]\n",
      " [2.50400281]\n",
      " [3.450415  ]\n",
      " [1.9750703 ]\n",
      " [1.17433752]\n",
      " [0.62633431]\n",
      " [2.52875851]\n",
      " [1.32897555]\n",
      " [2.40361296]\n",
      " [0.4599249 ]] \n",
      "\n",
      "Coefficient after model training: [[3.80663909]]\n"
     ]
    }
   ],
   "source": [
    "# Make training set and test set out of our dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\n",
    "lin_model = LinearRegressionWithGradientDescent()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "\n",
    "predicted_values = lin_model.predict(X_test)\n",
    "print(\"Predicted values based on model training:\\n\", predicted_values, \"\\n\")\n",
    "print(\"Actual values based on test set:\\n\", Y_test, \"\\n\")\n",
    "\n",
    "lin_model.get_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation <a class=\"anchor\" id=\"lr-evaluation\"></a>\n",
    "\n",
    "Know we need to know, how good are we with our model. For this we will use <b><a href=\"https://en.wikipedia.org/wiki/Root-mean-square_deviation\">Root mean squared error(RMSE)</a></b> and <b><a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\">Coefficient of Determination($R^2$ score)</a></b> to evaluate our model.\n",
    "\n",
    "$RMSE$ is defined as:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\dfrac{1}{m}\\sum_{i = 1}^{m}(h(x^i) - y^i)^2}$$\n",
    "\n",
    "$R^2$ is defined as:\n",
    "\n",
    "$$ R^2 = 1 - \\dfrac{SS_r}{SS_t} $$\n",
    "\n",
    "- $SS_r$ - is the total sum of errors if we take the mean of the observed values as the predicted value.\n",
    "- $SS_t$ - is the sum of the square of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.31653822319988595\n",
      "R2 score =  0.8988411204042933\n"
     ]
    }
   ],
   "source": [
    "# Let's define function that will calculate RMSE\n",
    "def get_rmse(p_values, t_values):\n",
    "        \"\"\" This function calculates RMSE based on model prediction and test set\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_values : array-like,\n",
    "                   predicted values by model after training\n",
    "        t_values : array-like\n",
    "                   actual values from test set\n",
    "        Returns\n",
    "        -------\n",
    "        Print-out of RMSE value\n",
    "        \"\"\"\n",
    "        \n",
    "        # ** is the same as pow() function. Just returns squared value.\n",
    "        mse = np.sum((p_values - t_values)**2)\n",
    "\n",
    "        # len(t_values) -> returns number of entries in array, this is 'm' value from equation\n",
    "        rmse = np.sqrt(mse/len(t_values))\n",
    "\n",
    "        print (\"RMSE = \", rmse)\n",
    "        return\n",
    "\n",
    "# Let's define function that will calculate coefficient of determination    \n",
    "def get_r2(p_values, t_values):\n",
    "    \"\"\" This function calculates R2 score based on model prediction and test set\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_values : array-like,\n",
    "               predicted values by model after training\n",
    "    t_values : array-like\n",
    "               actual values from test set\n",
    "    Returns\n",
    "    -------\n",
    "    Print-out of R2 score\n",
    "    \"\"\"\n",
    "    ss_r = np.sum((p_values - t_values)**2)\n",
    "    ss_t = np.sum((t_values - np.mean(t_values))**2)\n",
    "\n",
    "    r2_score = 1 - (ss_r/ss_t)\n",
    "    print (\"R2 score = \", r2_score)\n",
    "    return\n",
    "\n",
    "\n",
    "get_rmse(predicted_values, Y_test)\n",
    "get_r2(predicted_values, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLEARN solution <a class=\"anchor\" id=\"lr-sklearn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting coefficients from 'history':\n",
      "[[2.87133665]] \n",
      "\n",
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "RMSE is 0.2541310419919423\n",
      "R2 score is 0.9347970681170494\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make training set and test set\n",
    "sX_train, sX_test, sY_train, sY_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\n",
    "sklearn_lin_model = LinearRegression()\n",
    "history = sklearn_lin_model.fit(sX_train, sY_train)\n",
    "\n",
    "print(\"Getting coefficients from 'history':\")\n",
    "print(history.coef_, \"\\n\")\n",
    "\n",
    "y_test_predict = sklearn_lin_model.predict(sX_test)\n",
    "rmse = (numpy.sqrt(mean_squared_error(sY_test, y_test_predict)))\n",
    "r2 = r2_score(sY_test, y_test_predict)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results <a class=\"anchor\" id=\"lr-results\"></a>\n",
    "\n",
    "Here is a comparisment for results I got from my implementation defined above and the one used in SKLEARN library.\n",
    "\n",
    "| Value | My LR Implementation | SKLEARN LR Implementation   |\n",
    "|-------|----------------|-------------------|\n",
    "|coefficients |3.80663909|2.87133665 |\n",
    "|RMSE score|0.31653822319988595|0.2541310419919423|\n",
    "|R2 score|0.8988411204042933|0.9347970681170494|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources <a class=\"anchor\" id=\"lr-resources\"></a>\n",
    "\n",
    "- <a href=\"http://ocdevel.com/mlg/5\">Machine Learning Guide podcast, on Linear Regression</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=kHwlB_j7Hkc\">Andrew Ng lecture on Uni-variate Linear Regression</a>\n",
    "- <a href=\"https://www.udemy.com/data-science-linear-regression-in-python/\">Udemy course on Linear Regression in Python</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by <a class=\"reference external\" href=\"https://www.facebook.com/vladyslav.elashevskyy\">Vladyslav Ieliashevskyi</a> in May 2019, based on second lecture of Lviv Data Science School 2019, performed by <a class=\"reference external\" href=\"https://www.facebook.com/oleksandr.gurbych\">Oleksandr Gurbych</a>\n",
    "\n",
    "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/deed.en_US\">Creative Commons Attribution 3.0 Unported License</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
